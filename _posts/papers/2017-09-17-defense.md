---
layout: paper
categories: papers
permalink: papers/defense
title: Keeping the Bad Guys Out
---

# Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression
[Nilaksh Das][nilaksh], [Madhuri Shanbhogue][madhuri], [Shang-Tse Chen][shang], [Fred Hohman][fred], [Li Chen][li], [Michael E. Kounavis][michael], [Duen Horng Chau][polo]  

<figure>
     <img class="single" src="/images/papers/17-defense-arxiv.png">
    <figcaption class="single">
        A comparison of the classification results of an exemplar image from the German Traffic Sign Recognition Benchmark (GTSRB) dataset.
        A benign image (left) is originally classified as a <i>stop sign</i>, but after the addition of an adversarial perturbation to the image (middle) the resulting image is classified as a <i>max speed 100</i> sign.
        Using JPEG compression on the adversarial image (right), we recover the original classification of <i>stop sign</i>.

    </figcaption>
</figure>

## Abstract
Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. 
However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. 
These adversarial samples are crafted by adding small perturbations to normal, benign images. 
Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. 
In this work, we explore and demonstrate 
how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to  counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). 
An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. 
Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations.
Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.

## Materials
[PDF][17-defense-arxiv-pdf] | [BibTeX][17-defense-arxiv]

## Citation
**[Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression][17-defense-arxiv]**  
[Nilaksh Das][nilaksh], [Madhuri Shanbhogue][madhuri], [Shang-Tse Chen][shang], [Fred Hohman][fred], [Li Chen][li], [Michael E. Kounavis][michael], [Duen Horng Chau][polo]  
*arXiv:1705.02900. May 8, 2017.*  
<span class="paper-misc">
[PDF][17-defense-arxiv-pdf] | [BibTeX][17-defense-arxiv]
</span>

## BibTeX
```
@article{das2017keeping,
  title={Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression},
  author={Das, Nilaksh and Shanbhogue, Madhuri and Chen, Shang-Tse and Hohman, Fred and Chen, Li and Kounavis, Michael E and Chau, Duen Horng},
  journal={arXiv preprint arXiv:1705.02900},
  year={2017}
}
```

[nilaksh]: http://nilakshdas.com/ "Nilaksh Das"
[madhuri]: https://www.linkedin.com/in/madhuri-shanbhogue/ "Madhuri Shanbhogue"
[shang]: https://www.cc.gatech.edu/~schen351/ "Shang-Tse Chen"
[fred]: http://fredhohman.com "Fred Hohman"
[li]: https://www.linkedin.com/in/li-chen-phd-b2a10289/ "Li Chen"
[michael]: https://www.researchgate.net/profile/Michael_Kounavis "Michael E. Kounavis"
[polo]: http://www.cc.gatech.edu/~dchau/ "Polo Chau"

[17-defense-arxiv]: {{ site.url }}/papers/defense
[17-defense-arxiv-pdf]: https://arxiv.org/abs/1705.02900